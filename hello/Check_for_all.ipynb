{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEQ2SEQ_ATT:\n",
    "    def __init__( self,  n_hidden_size, n_class, lr, n_enc_size, n_dec_size, n_enc_vocab_size, n_dec_vocab_size, n_embedding_size ):\n",
    "        with tf.compat.v1.variable_scope('Input'):\n",
    "            self.lr = lr\n",
    "            self.n_class = n_class\n",
    "            self.n_enc_size = n_enc_size\n",
    "            self.n_dec_size = n_dec_size\n",
    "            self.n_hidden_size = n_hidden_size\n",
    "            self.n_enc_vocab_size = n_enc_vocab_size\n",
    "            self.n_dec_vocab_size = n_dec_vocab_size\n",
    "            self.n_embedding_size = n_embedding_size\n",
    "            \n",
    "            with tf.compat.v1.variable_scope('Placeholder'):\n",
    "                self.enc_input = tf.compat.v1.placeholder( tf.int64, [None, None], name='enc_input')\n",
    "                self.dec_input = tf.compat.v1.placeholder( tf.int64, [None, None], name='dec_input')\n",
    "                self.inf_input = tf.compat.v1.placeholder( tf.int64, [None, None], name='inf_input' ) \n",
    "                self.targets   = tf.compat.v1.placeholder( tf.int64, [None, None], name='tar_input')\n",
    "                self.x_seq_len = tf.compat.v1.placeholder( tf.int64, [None],       name=\"x_seq_len\")\n",
    "                self.y_seq_len = tf.compat.v1.placeholder( tf.int64, [None],       name=\"y_seq_len\")\n",
    "                self.dropout_keep = tf.compat.v1.placeholder( tf.float32,          name=\"dropout_keep\")\n",
    "\n",
    "            with tf.compat.v1.variable_scope('Variable'):\n",
    "                # enc_embeddings [ enc_voc_size, embedding_size ]\n",
    "                # dec_embeddings [ dec_voc_size, embedding_size ]\n",
    "                self.enc_embeddings = tf.Variable(tf.random_normal([ self.n_enc_vocab_size, self.n_embedding_size]), name='enc_embedding')\n",
    "                self.dec_embeddings = tf.Variable(tf.random_normal([ self.n_dec_vocab_size, self.n_embedding_size]), name='dec_embedding')\n",
    "                \n",
    "\n",
    "            with tf.compat.v1.variable_scope('MakeCell'):\n",
    "                self.enc_cell = tf.nn.rnn_cell.LSTMCell( num_units=self.n_hidden_size )\n",
    "                self.dec_cell = tf.nn.rnn_cell.LSTMCell( num_units=self.n_hidden_size )\n",
    "                self.enc_cell = tf.nn.rnn_cell.DropoutWrapper( self.enc_cell, output_keep_prob=self.dropout_keep )\n",
    "                self.dec_cell = tf.nn.rnn_cell.DropoutWrapper( self.dec_cell, output_keep_prob=self.dropout_keep )\n",
    "\n",
    "            with tf.compat.v1.variable_scope('Embedding'):\n",
    "                #enc_embed [ batch, seqlen, embedding_size ]\n",
    "                self.enc_embed = tf.nn.embedding_lookup( self.enc_embeddings, self.enc_input, name='enc_embed' ) # ( enc_voc_size, hidden )\n",
    "                self.dec_embed = tf.nn.embedding_lookup( self.dec_embeddings, self.dec_input, name='dec_embed' ) # ( dec_voc_size, hidden )\n",
    "\n",
    "       # enc_state  [ 2,     batch,  hidden ] context, hidden \n",
    "       # enc_outputs[ batch, seqlen, hidden ]\n",
    "        with tf.compat.v1.variable_scope('Encoder'):\n",
    "            self.enc_outputs, self.enc_state = \\\n",
    "            tf.nn.dynamic_rnn( self.enc_cell, self.enc_embed, sequence_length=self.x_seq_len, dtype=tf.float32 )\n",
    "            self.dec_state = self.enc_state\n",
    "\n",
    "\n",
    "        # dec_embed [ batch, seqlen, hidden ]\n",
    "        # context   [ batch, hidden         ]\n",
    "        with tf.compat.v1.variable_scope('Decoder'):\n",
    "            self.context = self.bahdanau_attention( self.enc_state, self.enc_outputs )\n",
    "            self.t_dec_embed = tf.transpose( self.dec_embed, [1, 0, 2] );\n",
    "            dec_idx=tf.constant(0)\n",
    "            dec_output_tensor = tf.TensorArray( tf.float32, size = self.n_dec_size )\n",
    "            def dec_cond( idx, p_state, enc_outputs, outupt_tensor, max_dec_size ):\n",
    "                return tf.less( idx, max_dec_size )\n",
    "        \n",
    "            def dec_body( idx, p_state, enc_outputs, dec_output_tensor, max_dec_size ):\n",
    "                i_dec_embed = tf.gather_nd(self.t_dec_embed, [[idx]])   \n",
    "                i_dec_embed = tf.transpose(i_dec_embed, [1, 0, 2] )   # [batch, 1, hidden]\n",
    "                context_expand = tf.expand_dims( self.context , 1)    # [batch, 1, hidden]\n",
    "                i_dec_embed_concat = tf.concat( [ context_expand, i_dec_embed], axis=-1 )  # [ batch, 1, hidden*2 ]\n",
    "                i_dec_outputs, i_dec_state = tf.nn.dynamic_rnn( self.dec_cell, i_dec_embed_concat, initial_state=p_state, dtype=tf.float32)\n",
    "                self.context = self.bahdanau_attention( i_dec_state, self.enc_outputs )\n",
    "                i_dec_outputs = tf.reshape( i_dec_outputs, [-1, self.n_hidden_size])\n",
    "                dec_output_tensor = dec_output_tensor.write( idx, i_dec_outputs )\n",
    "                return idx+1, i_dec_state, enc_outputs, dec_output_tensor, max_dec_size\n",
    "        \n",
    "\n",
    "        self.n_dec_state = tf.nn.rnn_cell.LSTMStateTuple(c=self.context, h=self.dec_state.h)\n",
    "        with tf.variable_scope('While'):\n",
    "            _, _, _, dec_output_tensor, _ = \\\n",
    "            tf.while_loop( cond = dec_cond, \n",
    "                           body = dec_body, \n",
    "                           loop_vars=[ dec_idx,\n",
    "                                       self.n_dec_state, \n",
    "                                       self.enc_outputs, \n",
    "                                       dec_output_tensor,\n",
    "                                       self.n_dec_size ] )\n",
    "\n",
    "            self.dec_outputs = dec_output_tensor.stack()\n",
    "            self.dec_outputs = tf.transpose(self.dec_outputs, [1, 0, 2] )\n",
    "            self.logits = tf.layers.dense( self.dec_outputs, self.n_class, activation=None, reuse=tf.AUTO_REUSE, name='output_dense')\n",
    "\n",
    "    \n",
    "        self.mask = tf.sequence_mask( self.y_seq_len, n_dec_size );\n",
    "        with tf.variable_scope('Loss'):\n",
    "           # targets [ batch, dec_voc_size ]\n",
    "            self.losses = tf.nn.sparse_softmax_cross_entropy_with_logits( logits=self.logits, labels=self.targets ) # losses =  [1, 32, 13]\n",
    "            self.t_loss =  tf.boolean_mask( self.losses, self.mask )  \n",
    "            self.loss = tf.reduce_mean( tf.boolean_mask( self.losses, self.mask )  )\n",
    "            self.optimizer  = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        with tf.variable_scope('Accuracy'):\n",
    "            self.prediction = tf.argmax(self.logits, 2, name='prediction', output_type=tf.int64 )\n",
    "            prediction_mask = self.prediction * tf.to_int64(self.mask)\n",
    "            correct_pred = tf.equal(prediction_mask, self.targets)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_pred, \"float\"), name=\"accuracy\")\n",
    "\n",
    "        with tf.variable_scope('While'):\n",
    "            def inf_cond( inf_idx, dec_input_idx, prev_state, output_tensor, max_dec_size ) :\n",
    "                return tf.less( inf_idx, max_dec_size )\n",
    "\n",
    "            def inf_body( inf_idx, dec_input_idx, prev_state, output_tensor, max_dec_size ) :\n",
    "                dec_input_embeddings = tf.nn.embedding_lookup( self.dec_embeddings, dec_input_idx )   # [ batch, 1, embedding ] [ \n",
    "                context_expand = tf.expand_dims(self.context, 1)                                      # [ batch, 1, hidden    ]\n",
    "                dec_input_embeddings = tf.concat( [ context_expand, dec_input_embeddings ], axis=-1 )\n",
    "                dec_outputs, dec_state = tf.nn.dynamic_rnn( self.dec_cell, dec_input_embeddings, sequence_length=[1], initial_state=prev_state, dtype=tf.float32)\n",
    "                self.context = self.bahdanau_attention( dec_state, self.enc_outputs )\n",
    "                logits = tf.layers.dense( dec_outputs, self.n_class, activation=None, reuse=tf.AUTO_REUSE, name='output_dense')\n",
    "                idx_prediction = tf.argmax( logits, 2, output_type=tf.int64, name='idx_prediction' )\n",
    "                output_tensor = output_tensor.write( inf_idx, idx_prediction )\n",
    "                return inf_idx+1, idx_prediction, dec_state, output_tensor, max_dec_size\n",
    "\n",
    "            inf_idx=tf.constant(0)\n",
    "            inf_output_tensor = tf.TensorArray( tf.int64, size = self.n_dec_size, name='inf_output_tensor' )\n",
    "            self.context = self.bahdanau_attention( self.enc_state, self.enc_outputs )\n",
    "            self.n_dec_state = tf.nn.rnn_cell.LSTMStateTuple(c=self.context, h=self.dec_state.h)\n",
    "\n",
    "            _, _, _, inf_output_tensor, _ = \\\n",
    "            tf.while_loop( cond = inf_cond, \n",
    "                           body = inf_body, \n",
    "                           loop_vars=[ inf_idx,\n",
    "                                       self.inf_input,\n",
    "                                       self.n_dec_state,\n",
    "                                       inf_output_tensor,\n",
    "                                       self.n_dec_size ])\n",
    "            self.inf_result = inf_output_tensor.stack()\n",
    "            self.inf_result = tf.reshape( self.inf_result, [-1], 'inf_result' ) \n",
    "\n",
    "\n",
    "    def bahdanau_attention( self, enc_state, enc_outputs ):\n",
    "        query = enc_state.h             # ( 2,     batch, hidden )\n",
    "        value = enc_outputs             # ( batch, seq,   hidden )\n",
    "        query_exp = tf.expand_dims(query, 1) # ( 2,     1,     hidden )\n",
    "        self.value_weight = tf.layers.dense(value,     self.n_hidden_size, activation=None, reuse=tf.AUTO_REUSE, name='value_weight')\n",
    "        self.query_weight = tf.layers.dense(query_exp, self.n_hidden_size, activation=None, reuse=tf.AUTO_REUSE, name='query_weight')\n",
    "        activation = tf.nn.tanh(self.value_weight + self.query_weight)\n",
    "        attention_score = tf.layers.dense(activation, 1, reuse=tf.AUTO_REUSE, name='attention_score')\n",
    "        attention_weight= tf.nn.softmax( attention_score )\n",
    "        self.context = tf.reduce_sum( attention_weight * value, axis=1, name='attention_context' ) # context = [batch, hidden]\n",
    "        return self.context;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
